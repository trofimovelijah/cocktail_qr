{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Импорт и определение библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# отключение параллелизма\n",
    "import os\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ставим разные пакеты оттуда\n",
    "!pip install langchain langchain-openai langchain-community openai tiktoken langchain-huggingface -q\n",
    "!pip install beautifulsoup4 pypdf sentence-transformers faiss-cpu unstructured pypandoc -q\n",
    "#huggingface_hub  langchain_experimental langchainhub\n",
    "#!pip install unstructured \"unstructured[pdf]\" -q\n",
    "!pip install sentence-transformers numpy transformers requests -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n",
      "/home/elijah/Documents/LLM/cocktail_qr/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# импортируем библиотеки\n",
    "from langchain.document_loaders import WebBaseLoader, PyPDFLoader, UnstructuredEPubLoader\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_text_splitters import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_huggingface import HuggingFaceEndpoint\n",
    "from langchain.agents import Tool, initialize_agent\n",
    "from langchain.prompts import PromptTemplate\n",
    "import tqdm as notebook_tqdm\n",
    "from huggingface_hub import HfApi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Готовим коктейль с помощью RAG\n",
    "## Чутка `embeddings` с `HuggingFace` для оснастки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sentence_transformers.SentenceTransformer:Load pretrained SentenceTransformer: sentence-transformers/paraphrase-multilingual-mpnet-base-v2\n"
     ]
    }
   ],
   "source": [
    "# Чтение токена из файла\n",
    "def read_token(file_path): \n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"HUGGINGFACEHUB_API_TOKEN\"):\n",
    "                return line.split('=')[1].strip()\n",
    "    raise ValueError(\"Token not found in the specified file.\")\n",
    "\n",
    "# Загрузка токена\n",
    "hf_token = read_token('../utils.key')\n",
    "\n",
    "# Формирование эмбеддингов\n",
    "hf_embeddings_model = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\",  \n",
    "    #\"sentence-transformers/paraphrase-multilingual-mpnet-base-v2\", \n",
    "    #\"cointegrated/LaBSE-en-ru\",\n",
    "    model_kwargs={\n",
    "        \"device\": \"cpu\",\n",
    "        \"token\": hf_token\n",
    "                  } \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Можно пропустить (если используете наполненную чашу векторного хранилища)\n",
    "### Немного `Document Loader`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pages count: 1\n",
      "max chars on page: 107261\n"
     ]
    }
   ],
   "source": [
    "# Загрузка EPUB файла от бармена-литературоведа\n",
    "loader = UnstructuredEPubLoader('../docs/Federle.epub', show_progress=True)\n",
    "doc_epub = loader.load()\n",
    "\n",
    "# смотрим, скока получилось загрузить\n",
    "print('pages count:', len(doc_epub))\n",
    "print('max chars on page:', max([len(page.page_content) for page in doc_epub]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загружаем книгу от крутого бармена\n",
    "#loader = WebBaseLoader(url)\n",
    "\"\"\"loader = PyPDFLoader('../docs/Bortnik_1000.pdf')\n",
    "data = loader.load()\n",
    "\n",
    "print('pages count:', len(data))\n",
    "print('max chars on page:', max([len(page.page_content) for page in data]))\"\"\"\n",
    "\n",
    "# на тот случай, если решим загружать подобную литературу пачкой pdf\n",
    "\"\"\"\n",
    "#!pip install unstructured \"unstructured[pdf]\" -q\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "loader = DirectoryLoader(\"../docs\", glob=\"**/*.pdf\", show_progress=True)\n",
    "data = loader.load()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Исполним `Text splitters` и разобьём на чанки"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Точная токенизация под `DeepSeek`**\n",
    "\n",
    "- Кастомный класс `TransformersTokenSplitter`:\n",
    "  - принимает токенизатор `Hugging Face`.\n",
    "  - токенизирует текст с помощью `tokenizer.encode()`.\n",
    "  - разбивает токены на чанки с учетом `chunk_size` и `chunk_overlap`.\n",
    "  - декодирует чанки обратно в текст через `tokenizer.decode()`.\n",
    "\n",
    "- Особенности:\n",
    "  - точное соответствие токенизации модели `DeepSeek`.\n",
    "  - сохранение перекрытия между чанками.\n",
    "  - поддержка любых токенизаторов из `Transformers`.\n",
    "\n",
    "- Почему это лучше стандартного `TokenTextSplitter`:\n",
    "  - точность: используется токенизатор, идентичный модели.\n",
    "  - гибкость: работает с любыми моделями `Hugging Face`.\n",
    "  - контроль: позволяет задавать параметры в токенах, а не символах.\n",
    "  \n",
    "Сравнение подходов:\n",
    "| Параметр | Решение 1 (Символы) | Решение 2 (Токены)|\n",
    "|---|---|---|\n",
    "| Точность | Средняя\t| Высокая |\n",
    "|Производительность\t | Быстрее\t| Медленнее (токенизация) |\n",
    "|Сложность\t| Просто\t| Требует токенизатор |\n",
    "|Рекомендация\t| Для общего случая\t| Для точного контекста |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import TextSplitter\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "class TransformersTokenSplitter(TextSplitter):\n",
    "    \"\"\"Кастомный сплиттер для токенизаторов Hugging Face.\"\"\"\n",
    "    \n",
    "    def __init__(self, tokenizer, chunk_size=256, chunk_overlap=64):\n",
    "        super().__init__(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "    def split_text(self, text: str) -> list[str]:\n",
    "        # Разбиваем текст на части, если он слишком большой\n",
    "        max_length = self.tokenizer.model_max_length\n",
    "        if len(text) > max_length:\n",
    "            # Разбиваем текст на части по max_length\n",
    "            parts = [text[i:i + max_length] for i in range(0, len(text), max_length)]\n",
    "        else:\n",
    "            parts = [text]\n",
    "        \n",
    "        chunks = []\n",
    "        for part in parts:\n",
    "            # Токенизируем часть текста\n",
    "            tokens = self.tokenizer.encode(part, add_special_tokens=False)\n",
    "            \n",
    "            # Разбиваем токены на чанки\n",
    "            current_chunk = []\n",
    "            current_length = 0\n",
    "            \n",
    "            for token in tokens:\n",
    "                current_chunk.append(token)\n",
    "                current_length += 1\n",
    "                \n",
    "                if current_length >= self._chunk_size:\n",
    "                    # Декодируем чанк обратно в текст\n",
    "                    chunk_text = self.tokenizer.decode(current_chunk)\n",
    "                    chunks.append(chunk_text)\n",
    "                    \n",
    "                    # Сохраняем перекрытие\n",
    "                    current_chunk = current_chunk[-self._chunk_overlap :]\n",
    "                    current_length = len(current_chunk)\n",
    "            \n",
    "            # Добавляем последний чанк\n",
    "            if current_chunk:\n",
    "                chunk_text = self.tokenizer.decode(current_chunk)\n",
    "                chunks.append(chunk_text)\n",
    "                \n",
    "        return chunks\n",
    "\n",
    "# -----------------------------------------------------------\n",
    "# Пример использования\n",
    "# -----------------------------------------------------------\n",
    "\n",
    "# Загрузка токенизатора DeepSeek\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"deepseek-ai/deepseek-llm-7b-chat\")\n",
    "\n",
    "# Инициализация сплиттера\n",
    "splitter = TransformersTokenSplitter(\n",
    "    tokenizer=tokenizer,\n",
    "    chunk_size=256,      # Размер чанка в токенах\n",
    "    chunk_overlap=64     # Перекрытие между чанками\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделяем книгу на части\n",
    "\n",
    "split_documents = []\n",
    "for page in doc_epub:\n",
    "    # Разбиваем текст страницы на чанки\n",
    "    chunks = splitter.split_text(page.page_content)\n",
    "    for chunk in chunks:\n",
    "        # Сохраняем метаданные страницы\n",
    "        split_documents += splitter.create_documents([page.page_content], metadatas=[page.metadata])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество чанков: 260\n",
      "Максимальное количество символов в чанке: 707\n"
     ]
    }
   ],
   "source": [
    "chunks = splitter.split_documents(doc_epub)\n",
    "print('Количество чанков:', len(chunks))\n",
    "print('Максимальное количество символов в чанке:', max([len(chunk.page_content) for chunk in chunks]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Наполним чашу `Vector Store`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:faiss.loader:Loading faiss with AVX512 support.\n",
      "INFO:faiss.loader:Successfully loaded faiss with AVX512 support.\n"
     ]
    }
   ],
   "source": [
    "# Если нужно сохранить новый документ в базу\n",
    "try:\n",
    "    db_embed = FAISS.from_documents(\n",
    "            split_documents, \n",
    "            hf_embeddings_model\n",
    "        )\n",
    "except Exception as e:\n",
    "    raise RuntimeError(f\"Ошибка при создании базы данных FAISS: {e}\")\n",
    "\n",
    "\n",
    "# Сохраняем векторную базу локально\n",
    "db_embed.save_local(\"../data/faiss_db_epub\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Нужно запустить (если используете готовое векторное хранилище)\n",
    "### Испьём чашу `Vector Store`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если нужно изъять из уже сохранённого документа из базы\n",
    "# Загрузка векторной базы данных из локального файла\n",
    "db_embed = FAISS.load_local(\n",
    "        \"../data/faiss_db_epub\", \n",
    "        hf_embeddings_model,\n",
    "        allow_dangerous_deserialization = True\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "НА ВОСЕМЬ ПОРЦИЙ:\n",
      "\n",
      "350 мл ржаного виски;\n",
      "\n",
      "120 мл ананасового сока;\n",
      "\n",
      "60 мл лимонного сока;\n",
      "\n",
      "1 л имбирного пива (истинно американский напиток, который непросто отыскать на наших просторах).\n",
      "\n",
      "Виски и соки заливаем в чашу для пунша, где уже лежит глыба льда. Вливаем имбирное пиво, трубим общий сбор – пора развеять экзистенциальную тоску!\n",
      "\n",
      "Приключения Шербета Холмса\n",
      "\n",
      "ПРИКЛЮЧЕНИЯ ШЕРЛОКА ХОЛМСА (1891–1892)\n",
      "\n",
      "СЭР АРТУР КОНАН ДОЙЛЬ\n",
      "30 мл водки;\n",
      "\n",
      "200 г нарезанного кубиками арбуза;\n",
      "\n",
      "10 мл лимонного сока;\n",
      "\n",
      "½ чайной ложки (2,5 г) сахара;\n",
      "\n",
      "15 мл дынного ликера.\n",
      "\n",
      "Водку, арбуз, лимонный сок, сахар и пригоршню льда помещаем в блендер и доводим до однородности. Переливаем в коктейльный бокал, аккуратно добавляем ликер. Глоток чудо-смузи подарит вам такую отвагу, что вы не побоитесь бросить вызов даже самому Главноуправителю.\n",
      "\n",
      "Коктейль о двух городах\n",
      "\n",
      "ПОВЕСТЬ О ДВУХ ГОРОДАХ (1859)\n",
      "\n",
      "ЧАРЛЬЗ ДИККЕНС\n",
      "60 мл клюквенного сока;\n",
      "\n",
      "60 мл апельсинового сока;\n",
      "\n",
      "50 мл светлого рома;\n",
      "\n",
      "30 мл кокосового рома;\n",
      "\n",
      "1 чайная ложка гранатового сиропа;\n",
      "\n",
      "долька апельсина или ломтик ананаса для украшения (на выбор).\n",
      "\n",
      "Ингредиенты взбалтываем в шейкере со льдом – очень может быть, что все содержимое окрасится в кроваво-красный, – и вместе со льдом выливаем в стакан коллинз. Декорируем дарами тропиков – ананасом, апельсином, поросячьим глазом…\n",
      "\n",
      "Бесконечная цедра\n",
      "\n",
      "БЕСКОНЕЧНАЯ ШУТКА (1996)\n",
      "\n",
      "ДЭВИД ФОСТЕР УОЛЛЕС\n",
      "3 свежих вымытых ягоды земляники;\n",
      "\n",
      "8 свежих тщательно промытых веточек мяты;\n",
      "\n",
      "15 мл лимонного сока;\n",
      "\n",
      "30 мл сиропа агавы;\n",
      "\n",
      "50 мл светлого рома;\n",
      "\n",
      "сильногазированная вода для долива.\n",
      "\n",
      "В стакане коллинз толчем мадлером ягоды, мяту, сок и сироп. Добавляем две пригоршни льда и ром, тщательно перемешиваем, доверху доливаем газировкой. Замираем в ожидании, когда прольются слезы радости.\n",
      "\n",
      "Лайм для старого морехода\n",
      "\n",
      "СКАЗАНИЕ О СТАРОМ МОРЕХОДЕ (1798)\n",
      "\n",
      "СЭМЮЭЛ ТЕЙЛОР КОЛЬРИДЖ\n",
      "60 мл бренди или коньяка;\n",
      "\n",
      "15 мл оршада – сиропа из миндального молока, сахара и апельсина;\n",
      "\n",
      "2 дэша (дважды по чуть-чуть) биттера «Ангостура».\n",
      "\n",
      "Самый подходящий напиток для того дня, когда вам сообщат, что ваш богатенький родственник сыграл в ящик упокоился с миром. Ингредиенты взбалтываем в шейкере со льдом, процеживаем в коктейльный бокал – и, собравшись с духом, прямо спрашиваем, упомянуты ли мы в завещании.\n",
      "\n",
      "Джин Эйр\n",
      "\n",
      "ДЖЕЙН ЭЙР (1847)\n",
      "\n",
      "ШАРЛОТТА БРОНТЕ\n"
     ]
    }
   ],
   "source": [
    "query = \"ржаной виски, грейпфрутовый сок\"\n",
    "results = db_embed.similarity_search(query, k=5)\n",
    "for result in results:\n",
    "    print(result.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Месье системный промт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Системный промпт\n",
    "system_prompt = \"\"\"Ты — помощник для поиска рецептов коктейлей. Используй только данные из контекста ниже:\n",
    "        {context}\n",
    "\n",
    "        Пример:\n",
    "        Пользователь: Какие коктейли можно сделать из водки и апельсинового сока?\n",
    "        Агент: Использую инструмент \"Cocktail Recipe Finder\" для поиска рецептов. Вот что найдено: [рецепт].\n",
    "\n",
    "        **Строгий формат ответа**:\n",
    "        1. Название коктейля (только одно уникальное название из источника)\n",
    "        2. Состав:\n",
    "        - Ингредиент 1 (объем)\n",
    "        - Ингредиент 2 (объем)\n",
    "        3. Способ приготовления:\n",
    "        - Шаг 1\n",
    "        - Шаг 2\n",
    "\n",
    "        **Запрещено**:\n",
    "        - Добавлять описание вкуса, аромата или эмоций\n",
    "        - Использовать сложные термины (например, \\\"вкусовые соединения\\\")\n",
    "        - Создавать варианты в скобках\n",
    "\n",
    "        Пример корректного ответа по введённым ингредиентам (ржаной виски, грейпфрутовый сок):\n",
    "        Коктейль \"Рожь и предубеждение\"\n",
    "        Состав:\n",
    "        - Ржаной виски (50 мл)\n",
    "        - Грейпфрутовый сок (90 мл)\n",
    "        Способ приготовления:\n",
    "        - Ингредиенты выливаем в стакан рокс поверх кубиков льда\"\"\"\n",
    "\n",
    "# Промпт для LangChain\n",
    "qa_prompt = PromptTemplate(\n",
    "    template=system_prompt + \"\\nВопрос: {question}\\nОтвет:\",\n",
    "    input_variables=[\"context\", \"question\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Полирнём `Retriver` и инициализирует `llm` для вкуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain ollama -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Инициализация модели DeepSeek\n",
    "from langchain.llms import Ollama\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "# Инициализация модели через Ollama\n",
    "llm = Ollama(\n",
    "    model=\"deepseek-llm:7b-chat\",\n",
    "    temperature=0.3,        # Увеличиваем для большей креативности\n",
    "    num_predict=512,        # Уменьшаем длину ответа\n",
    "    repeat_penalty=1.2,     # Усиливаем штраф за повторы\n",
    "    #top_p=0.9,              # Для фокусировки\n",
    "    #top_k=40,               # Ограничиваем выбор токенов\n",
    "    stop=[\"\\\\n\\\\n\"]         # Остановка при двойных переносах\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Модифицируем параметры ретривера\n",
    "retriever = db_embed.as_retriever(\n",
    "    search_type=\"mmr\",              # Maximal Marginal Relevance\n",
    "    search_kwargs={\n",
    "        \"k\": 3,                     # Уменьшаем количество возвращаемых документов\n",
    "        \"fetch_k\": 10,              # Уменьшаем общий пул для поиска\n",
    "        \"lambda_mult\": 0.6          # Баланс между релевантностью и разнообразием\n",
    "    }\n",
    ")\n",
    "\n",
    "\n",
    "# Инициализация цепочки RetrievalQA\n",
    "retrieval_qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    retriever=retriever,  # Ваш ретривер (например, FAISS)\n",
    "    chain_type_kwargs={\"prompt\": qa_prompt},\n",
    "    return_source_documents=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Миксуем рецепт коктейля `agent` 00х"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Создание инструмента\n",
    "agent_tools = [\n",
    "    Tool(\n",
    "        name=\"Cocktail Recipe Finder\",\n",
    "        func=retrieval_qa.run,\n",
    "        description=\"Используй этот инструмент, чтобы найти рецепты коктейлей по заданным ингредиентам. Вводи список ингредиентов, и инструмент вернёт подходящие рецепты. Пример: ['сок', 'виски'].\"\n",
    "    )\n",
    "]\n",
    "\n",
    "# создание агента\n",
    "agent = initialize_agent(\n",
    "    tools=agent_tools, \n",
    "    llm=llm, \n",
    "    agent=\"conversational-react-description\",       #\"zero-shot-react-description\", \"plan-and-execute\"\n",
    "    verbose=True,\n",
    "    system_prompt=system_prompt                     # не забыть указать системный промт для вкуса\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Задаём общие параметры, после чего коктейль готов\n",
    "## Вначале коктейль"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Объявление функции на ввод данных от пользователя\n",
    "def get_user_input():\n",
    "    \"\"\"\n",
    "    Запрашивает у пользователя ингредиенты для поиска рецептов.\n",
    "    \"\"\"\n",
    "    user_input = input(\"Введите ингредиенты, разделенные запятыми: \")\n",
    "    ingredients = [ingredient.strip() for ingredient in user_input.split(\",\")]\n",
    "    return ingredients\n",
    "\n",
    "\n",
    "# Обработка ответа\n",
    "def format_response(response):\n",
    "    if \"не знаю\" in response.lower() or \"не найдено\" in response.lower():\n",
    "        return \"К сожалению, я не нашёл коктейлей с этими ингредиентами. Попробуйте другие ингредиенты.\"\n",
    "    else:\n",
    "        return response\n",
    "    \n",
    "\n",
    "# Функция выбора стиля ответа\n",
    "def choose_style(styles):\n",
    "    chosen_style = input(\"Выберите номер стиля (по умолчанию 4): \") or \"4\"\n",
    "    return styles.get(chosen_style, \"по-умолчанию\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Бурбон в летнюю ночь (ок. 1600)\n",
      "    * Шеекер\n",
      "      - 50 мл дешевого виски; сплэш ананасового сока (это означает плеснуть 5–10 мл); шлепок кокосовых сливок.\n",
      "      - Ингредиенты наливаем поверх кубиков льда в стакан рокс (впрочем, подойдет и туристическая кружка). Энергично перемешиваем, хватаем с полки \"Таитянский для начинающих\" и мчим в аэропорт!\n",
      "2. Прощай, \"Амаретто\"! (1929)\n",
      "    * Шеекер\n",
      "      - 60 мл водки; 15 мл сока лайма; ½ чайной ложки (2,5 г) вустерского соуса; ½ чайной ложки (2,5 г) васаби; 12–15 капель острого соуса (3 дэша на языке барменов); соль и перец по вкусу.\n",
      "      - Все ингредиенты соединяем в шейкер с льдом. От души взбалтываем и процеживаем через стрейнер в стакан коллинз на свежеприготовленный лед. Поборники традиций украшают напиток стебельком сельдерея, но если вы любите \"погорячее\", то просто удвойте порцию васаби.\n",
      "3. Говардс-бленд (1910)\n",
      "    * Форстэсер\n",
      "      - Эдвин Морган Фостер\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    # Стилевые инструкции с акцентом на общие характеристики\n",
    "    style_instructions = {\n",
    "        \"1\": {\n",
    "            \"name\": \"Космический ужас Лавкрафта\",\n",
    "            \"description\": (\n",
    "                \"Используй архаичную лексику, метафоры древних сил и непостижимых существ. \"\n",
    "                \"Допустимые элементы: нектар тьмы, дрожащие ингредиенты, запретные ритуалы. \"\n",
    "                \"Избегай бытовых описаний.\"\n",
    "            ),\n",
    "            \"params\": {\"temperature\": 0.8, \"top_p\": 0.85}\n",
    "        },\n",
    "        \"2\": {\n",
    "            \"name\": \"Гопническо-быдляцкий жаргон\",\n",
    "            \"description\": (\n",
    "                \"Используй грубую лексику, просторечия и абсурдные сравнения. \"\n",
    "                \"Примеры: 'замути', 'вмазать', 'как пацанчик'. \"\n",
    "                \"Сокращай предложения.\"\n",
    "            ),\n",
    "            \"params\": {\"temperature\": 0.9, \"top_p\": 0.95}\n",
    "        },\n",
    "        \"3\": {\n",
    "            \"name\": \"Экспериментальный стиль Берроуза\",\n",
    "            \"description\": (\n",
    "                \"Разрывай текст на фрагменты, используй нелинейность и аллюзии. \"\n",
    "                \"Пример: 'Шейкер*вибрация*льдинки-глаза*взрыв*смешение'.\"\n",
    "            ),\n",
    "            \"params\": {\"temperature\": 1.0, \"top_p\": 0.99}\n",
    "        },\n",
    "        \"4\": {\n",
    "            \"name\": \"Стандартный\",\n",
    "            \"description\": \"Без изменений\",\n",
    "            \"params\": {\"temperature\": 0.0, \"top_p\": 0.95}\n",
    "        }\n",
    "    }\n",
    "\n",
    "    while True:\n",
    "        ingredients = get_user_input()\n",
    "        if not ingredients:\n",
    "            print(\"Вы не ввели ингредиенты. Попробуйте снова.\")\n",
    "            continue\n",
    "\n",
    "        # Базовый запрос\n",
    "        ingredients_str = \", \".join(ingredients)\n",
    "        user_prompt = f\"\"\"\n",
    "        Найди рецепты коктейлей по ингредиентам: {ingredients_str}\n",
    "        Учти соответствие с системным промтом и стилистикой ответа.\n",
    "        \"\"\"\n",
    "\n",
    "        response = retrieval_qa.invoke({\"query\": user_prompt})\n",
    "        original_response = response[\"result\"]\n",
    "\n",
    "        # Выбор стиля\n",
    "        chosen_style = input(\"Выберите стиль (1-4): \") or \"4\"\n",
    "        style_config = style_instructions.get(chosen_style, style_instructions[\"4\"])\n",
    "        \n",
    "        # Промпт для стилизации\n",
    "        style_prompt = f\"\"\"Перепиши этот текст в заданном стиле: {original_response}\\nСтиль: {style_instructions}.\\nРезультат:\\n''\n",
    "        \"\"\"\n",
    "\n",
    "        # Генерация с настройками стиля\n",
    "        styled_response = llm.invoke(\n",
    "            style_prompt,\n",
    "            temperature=style_config[\"params\"][\"temperature\"],\n",
    "            top_p=style_config[\"params\"][\"top_p\"]\n",
    "        )\n",
    "\n",
    "        # Постобработка\n",
    "        styled_response = styled_response.strip()\n",
    "\n",
    "        print(styled_response)\n",
    "        break\n",
    "\n",
    "    return styled_response\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Сохраняем результат в переменную\n",
    "    recipe = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Теперь озвучка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "# импортируем библиотеки\n",
    "import requests\n",
    "import uuid\n",
    "\n",
    "# функция для получения токена\n",
    "def get_token(auth_token, scope='SALUTE_SPEECH_PERS'):\n",
    "    \"\"\"\n",
    "      Выполняет POST-запрос к эндпоинту, который выдает токен.\n",
    "\n",
    "      Параметры:\n",
    "      - auth_token (str): токен авторизации, необходимый для запроса.\n",
    "      - область (str): область действия запроса API. По умолчанию — «SALUTE_SPEECH_PERS».\n",
    "\n",
    "      Возвращает:\n",
    "      - ответ API, где токен и срок его \"годности\".\n",
    "      \"\"\"\n",
    "    # Создадим идентификатор UUID (36 знаков)\n",
    "    rq_uid = str(uuid.uuid4())\n",
    "\n",
    "    # API URL\n",
    "    url = \"https://ngw.devices.sberbank.ru:9443/api/v2/oauth\"\n",
    "\n",
    "    # Заголовки\n",
    "    headers = {\n",
    "        'Content-Type': 'application/x-www-form-urlencoded',\n",
    "        'RqUID': rq_uid,\n",
    "        'Authorization': f'Basic {auth_token}'\n",
    "    }\n",
    "\n",
    "    # Тело запроса\n",
    "    payload = {\n",
    "        'scope': scope\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        # Делаем POST запрос с отключенной SSL верификацией\n",
    "        # (можно скачать сертификаты Минцифры, тогда отключать проверку не надо)\n",
    "        response = requests.post(url, headers=headers, data=payload, verify=False)\n",
    "        return response\n",
    "    except requests.RequestException as e:\n",
    "        print(f\"Ошибка: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Прочитанное значение: 'MTVjODUyZDMtZTU2Mi00ZWY5LWI3YzctZGY4YjUxZGMzMjhjOjllZmQ3NjZmLTVhYWQtNGYxOC1iN2I4LTQwOGMyY2IwZWI4Yg=='\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elijah/Documents/LLM/cocktail_qr/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'ngw.devices.sberbank.ru'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Считываем ключ авторизации из файла\n",
    "with open('../utils.key', 'r') as file:\n",
    "    for line in file:\n",
    "        if line.startswith('SBER_API_KEY'):\n",
    "            SBER_API_KEY = line.split('=', 1)[1].strip(' \\n\\r')\n",
    "            if not SBER_API_KEY.endswith('=='):  # Если нет ==, добавляем их\n",
    "                SBER_API_KEY += '=='\n",
    "            break\n",
    "\n",
    "print(f\"Прочитанное значение: '{SBER_API_KEY}'\")\n",
    "#'MTVjODUyZDMtZTU2Mi00ZWY5LWI3YzctZGY4YjUxZGMzMjhjOjllZmQ3NjZmLTVhYWQtNGYxOC1iN2I4LTQwOGMyY2IwZWI4Yg=='\n",
    "\n",
    "response = get_token(SBER_API_KEY) # получаем токен\n",
    "if response != None:\n",
    "    salute_token = response.json()['access_token']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# функция для синтеза речи\n",
    "def synthesize_speech(text, token, format='wav16', voice='May_24000'):\n",
    "    url = \"https://smartspeech.sber.ru/rest/v1/text:synthesize\" # URL для синтеза речи\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {token}\", # токен авторизации\n",
    "        \"Content-Type\": \"application/text\" # тип содержимого\n",
    "    }\n",
    "    params = {\n",
    "        \"format\": format, # формат аудио\n",
    "        \"voice\": voice # голос\n",
    "    }\n",
    "    response = requests.post(\n",
    "            url, \n",
    "            headers=headers, \n",
    "            params=params, \n",
    "            data=text.encode(), # текст для синтеза\n",
    "            verify=False # отключение проверки SSL\n",
    "        )\n",
    "\n",
    "    # проверка на успешность синтеза\n",
    "    if response.status_code == 200:\n",
    "        # Сохранение синтезированного аудио в файл\n",
    "        with open('output.wav', 'wb') as f:\n",
    "            f.write(response.content)\n",
    "        print(\"Аудио успешно синтезировано и сохранено как 'output.wav'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/elijah/Documents/LLM/cocktail_qr/.venv/lib/python3.12/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'smartspeech.sber.ru'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Аудио успешно синтезировано и сохранено как 'output.wav'\n"
     ]
    }
   ],
   "source": [
    "# вызов функции для синтеза речи\n",
    "synthesize_speech(recipe, salute_token)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
