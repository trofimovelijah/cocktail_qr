{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain langchain_experimental langchain-openai openai tiktoken huggingface_hub langchain-huggingface -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# для использования ключа из LLM_bot\n",
    "!wget https://raw.githubusercontent.com/a-milenkin/LLM_practical_course/main/notebooks/utils.py -q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Выбор модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Если используете ключ из курса, запустите эту ячейку\n",
    "from utils import ChatOpenAI\n",
    "\n",
    "#course_api_key= \"Введите ваш ключ, полученный в боте курса\"\n",
    "course_api_key = getpass(prompt=\"Введите ваш ключ курса\")\n",
    "\n",
    "# инициализируем языковую модель\n",
    "llm = ChatOpenAI(temperature=0.0, course_api_key=course_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from langchain.chains import LLMChain\n",
    "#from langchain_huggingface import HuggingFaceEndpoint\n",
    "#from langchain.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# если хотим использовать модель с HuggingFace\n",
    "#os.environ['HUGGINGFACEHUB_API_TOKEN'] = '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'\n",
    "\n",
    "# Создаем объект языкового модели\n",
    "#llm = HuggingFaceEndpoint(\n",
    "#      repo_id=\"mistralai/Mistral-7B-Instruct-v0.3\",\n",
    "#      task=\"text-generation\",  # Вид задачи, в нашем случае - генерация текста\n",
    "#    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Задаём общие параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Запрашиваем у пользователя ввод ингредиентов\n",
    "user_input = input(\"Введите ингредиенты, разделенные запятыми: \")\n",
    "\n",
    "# Преобразуем введенную строку в список, удаляя лишние пробелы\n",
    "ingredients = [ingredient.strip() for ingredient in user_input.split(\",\")]\n",
    "\n",
    "# Форматируем строку с использованием join для соединения элементов массива\n",
    "ingredients_str = \", \".join(ingredients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Напиши, какие коктейли ты можешь изготовить из представленных ниже ингредиентов. Также укажи, как ещё составляющие входят в коктейли и в какой пропорции изготавливаются. \n",
      "Дополнительно укажи, какие ингредиенты нужно добавить, чтобы получились другие коктейли. \n",
      "Ингредиенты: виски, вишня\n"
     ]
    }
   ],
   "source": [
    "# Задаем системный промт\n",
    "system_prompt = \"Ты опытный бармен, специализирующийся на алкогольных коктейлях.\"\n",
    "\n",
    "# Задаем промт пользователя\n",
    "user_prompt = f\"Напиши, какие коктейли ты можешь изготовить из представленных ниже ингредиентов. Также укажи, как ещё составляющие входят в коктейли и в какой пропорции изготавливаются. \\nДополнительно укажи, какие ингредиенты нужно добавить, чтобы получились другие коктейли. \\nИнгредиенты: {ingredients_str}\"\n",
    "\n",
    "# Выводим сформированный промт\n",
    "print(user_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Получаем рецепт"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12644/2415783521.py:8: LangChainDeprecationWarning: The method `BaseChatModel.__call__` was deprecated in langchain-core 0.1.7 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = llm(messages).content\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Из представленных ингредиентов - виски и вишня - можно приготовить несколько вкусных коктейлей:\n",
      "\n",
      "1. Виски-содовая с вишней:\n",
      "   - 60 мл виски\n",
      "   - 120 мл содовой воды\n",
      "   - 2-3 вишни\n",
      "   - Лед\n",
      "   - В высокий стакан кладем вишни, затем добавляем лед.\n",
      "   - Заливаем виски и содовую воду.\n",
      "   - Перемешиваем и украшаем вишней.\n",
      "\n",
      "2. Виски-сауэр с вишней:\n",
      "   - 60 мл виски\n",
      "   - 30 мл свежего лимонного сока\n",
      "   - 15 мл сахарного сиропа\n",
      "   - 2-3 вишни\n",
      "   - Лед\n",
      "   - В шейкер кладем вишни и разминаем их.\n",
      "   - Добавляем виски, лимонный сок, сахарный сироп и лед.\n",
      "   - Хорошо взбалтываем и процеживаем в бокал.\n",
      "   - Украшаем вишней.\n",
      "\n",
      "Для приготовления других коктейлей с этими ингредиентами можно добавить, например, апельсиновый сок для коктейля \"Виски-смешанный с апельсином\" или имбирный сироп для коктейля \"Виски-мулл\".\n"
     ]
    }
   ],
   "source": [
    "# Создаем список сообщений\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]\n",
    "\n",
    "# Получаем ответ от модели\n",
    "response = llm(messages).content\n",
    "\n",
    "# Выводим ответ\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Из представленных ингредиентов (виски и вишня) можно приготовить несколько классических коктейлей:\n",
      "\n",
      "1. Виски-содовая:\n",
      "- 60 мл виски\n",
      "- 120 мл содовой воды\n",
      "- Лед\n",
      "- Лимонная цедра (для украшения)\n",
      "\n",
      "2. Виски-кола:\n",
      "- 60 мл виски\n",
      "- 120 мл колы\n",
      "- Лед\n",
      "- Лимонная цедра (для украшения)\n",
      "\n",
      "3. Виски-сауэр:\n",
      "- 60 мл виски\n",
      "- 30 мл свежевыжатого лимонного сока\n",
      "- 15 мл сахарного сиропа\n",
      "- Лед\n",
      "- Вишня (для украшения)\n",
      "\n",
      "Для приготовления других коктейлей можно добавить следующие ингредиенты:\n",
      "\n",
      "- Для коктейля \"Манхэттен\" добавьте к виски и вишне также вермут и ангостуру биттер. Пропорции: 60 мл виски, 30 мл вермута, 2-3 капли биттера, вишня для украшения.\n",
      "- Для коктейля \"Олд-фэшнед\" помимо виски и вишни понадобится сахарный кубик, ангостура биттер и апельсиновая цедра. Пропорции: 60 мл виски, 1 сахарный кубик, 2-3 капли биттера, апельсиновая цедра, вишня для украшения.\n",
      "\n",
      "Надеюсь, эти рецепты помогут вам создать вкусные коктейли для ваших гостей!\n"
     ]
    }
   ],
   "source": [
    "# Если выбрали модель с HuggingFace\n",
    "# Объединяем системный и пользовательский промты в одну строку\n",
    "full_prompt = f\"{system_prompt}\\n{user_prompt}\"\n",
    "\n",
    "# Получаем ответ от модели\n",
    "response = llm(full_prompt).content\n",
    "\n",
    "# Выводим ответ\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Цепочки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# создадим шаблон и промпт\n",
    "template = '''Перепиши этот текcт в заданном стиле: {output_text}\n",
    "Стиль: {style}.\n",
    "Результат:'''\n",
    "\n",
    "prompt = PromptTemplate(input_variables=['output_text', 'style'], template=template)\n",
    "\n",
    "style_changer_chain = prompt | llm # И ВСЁ!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = response\n",
    "style = 'Роман 18 века'\n",
    "\n",
    "answer = style_changer_chain.invoke({'output_text': text, 'style': style}).content\n",
    "print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
